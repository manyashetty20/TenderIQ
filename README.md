# ğŸ¤– TenderIQ

**TenderIQ** is an intelligent document understanding platform for tenders. It helps teams extract, search, and query large tender documents using modern LLMs.

---

## ğŸ‘‹ Overview

TenderIQ lets you:

- ğŸ“„ Upload tender PDFs or TXT files per project
- ğŸ§¾ Parse and chunk large documents into manageable blocks
- ğŸ¤– Embed using **Groq** or **LLaMA** for vector similarity search
- ğŸ’¬ Ask natural language questions and receive context-rich answers
- ğŸ—‚ Organize and reuse processing results project-wise

The platform combines a **Streamlit frontend** with a **FastAPI backend**, along with PyMuPDF-based parsing and LLM-powered embeddings.

---

## ğŸš€ Quickstart Setup

```bash
# 1. Clone the repo
git clone https://github.com/manyashetty20/TenderIQ.git
cd TenderIQ

# 2. Create & activate virtual environment
python3 -m venv env
source env/bin/activate  # or env\Scripts\activate (Windows)

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set your Groq API key
cp .env.example .env
# Edit .env and add GROQ_API_KEY=your_key

# ğŸ¤– TenderIQ

**TenderIQ** is an intelligent document understanding platform for tenders. It helps teams extract, search, and query large tender documents using modern LLMs.
```
## ğŸ§  Model Requirements

If using the **LLaMA model** locally, download the appropriate weights from Hugging Face:

ğŸ”— [Llama-2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_K_M.gguf)

After downloading, configure to the model path.


## ğŸ—ï¸ Environment Variables
| Variable        | Description              |
|-----------------|--------------------------|
| `GROQ_API_KEY`  | API key for Groq cloud   |
| `LLAMA_MODEL`   | (optional) Path/ID of local LLaMA checkpoint |
---

## ğŸ’½ Running the App

### Start FastAPI Backend
```bash
uvicorn src.api.main:app --reload
```

### Start Streamlit Frontend
```bash
streamlit run app.py
```

Visit [http://localhost:8501](http://localhost:8501) to start using TenderIQ.

---

## ğŸ§  Usage Guide

1. **Project Selection**
   - Select or create a new tender project from the sidebar

2. **Upload Document**
   - Upload `.pdf` or `.txt` file (stored locally)

3. **Choose Model**
   - Use sidebar radio buttons:
     - ğŸ§  **Groq** (cloud)
     - ğŸ¦™ **LLaMA** (local)

4. **Process Document**
   - ğŸ” `Parse` â†’ extract raw text
   - ğŸ§© `Chunk` â†’ split into overlapping text blocks
   - ğŸ§  `Embed` â†’ generate embeddings
   - ğŸ’¾ `Save Index`

5. **Ask Questions**
   - Type any question related to the uploaded document
   - See LLM-based answers powered by your selected model

6. **Extract Tasks**
   - Get all tasks from documents

---

## ğŸ—‚ Project Structure

```
TenderIQ/
â”‚
â”œâ”€â”€ app.py                     # Streamlit frontend entry-point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/               # Uploaded files per project
â”‚   â””â”€â”€ projects.json          # Metadata about all projects
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                   # FastAPI routes for processing 
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”œâ”€â”€ parse.py
â”‚   â”‚   â”œâ”€â”€ chunk.py
â”‚   â”‚   â”œâ”€â”€ embed.py
â”‚   â”‚   â””â”€â”€ query.py
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”œâ”€â”€ parser.py          # PDF/TXT parsing using PyMuPDF
â”‚   â”‚   â””â”€â”€ chunker.py
â”‚   â”‚
â”‚   â”œâ”€â”€ embedding/
â”‚   â”‚   â”œâ”€â”€ model.py           # Handles embedding via LLaMA/Groq
â”‚   â”‚   â””â”€â”€ index.py           # Save/load embedding index
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_api.py
```


## ğŸ› ï¸ Tech Stack
| Layer        | Technology |
|--------------|------------|
| Frontend     | Streamlit  |
| Backend      | FastAPI    |
| Parsing      | PyMuPDF    |
| LLMs         | Groq, LLaMA|
| Embeddings   | FAISSâ€‘style store |
| Language     | Pythonâ€¯3.9+|



---

## ğŸ“Œ Notes

- Text extraction uses **PyMuPDF** for precise layout-based parsing
- Embeddings are stored per project for reuse and efficiency
- LLMs are modular â€“ add your own with minor changes in `embedding/model.py`

---

## ğŸ“¤ Artifacts & Docs

| Dataset Uploads | Embedding Index | Models      | Docs              |
|-----------------|------------------|-------------|-------------------|
| `data/uploads`  | `data/index/`    | Groq, LLaMA | ğŸ“„ User Guide (TBD)|

---

## ğŸ™Œ Contributions

We welcome ideas and feedback! Please open issues or pull requests for:

- Model integrations
- UI/UX improvements
- Vector store enhancements


---

## ğŸ‘©â€ğŸ’» Author

**Manya Shetty** |
**Riddhi Samitha** | 
**Md Kalim**   
Intern @ United Telecoms Ltd.  
GitHub: [@manyashetty20](https://github.com/manyashetty20)
